evaluate_synthesis_data_task:
  description: >
    Compare synthesis data from the test dataset against the ground truth 
    and make binary decisions (1 for match, 0 for no match) for each component. 
    Your evaluation must focus on these specific criteria:

    1. For 'method': Decide if the synthesis methods match (1) or not (0)
    2. For 'precursors': For each precursor, decide if it's correctly identified (1) or not (0)
       Note: The order of precursors doesn't matter, only whether each ground truth precursor is correctly identified in the test data. Keys can be either in chemical formula or chemical name - you need to understand the meaning of the keys.
    3. For 'characterization_techniques': For each technique, decide if it's correctly identified (1) or not (0)
       Note: The order of techniques doesn't matter, only whether each ground truth technique is correctly identified in the test data. Keys can be either in instrument name or short name of the characterization process - you need to understand the meaning of the keys.
    4. For 'steps': Decide if the synthesis procedure steps match or not. The match value should be a float between 0.00 and 1.00 and it should be round up to two decimal points.
       Note: This is a holistic judgment about whether the test steps capture the same procedure as the ground truth steps.

    For each component, provide your binary decision (1 for match, 0 for no match).

    You must return your evaluation results in the following exact format (Use only {} JSON data, don't use markdown ```json format):
    {
      "synthesis_data": {
        "method": {
          "match_value": 1 or 0,
          "reference": "ground truth method",
          "test": "test method"
        },
        "precursors": {
          "reference": ["precursor1", "precursor2"],
          "test": ["precursor1", "precursor3"],
          "matches": [
            {
              "reference_item": "precursor1",
              "test_item": "precursor1",
              "match_value": 1 or 0
            }
          ],
          "total_ground_truth_items": total number of precursors in ground truth data,
          "total_match": total number of matched precursors,
          "missing_items": ["precursor2"],
          "extra_items": ["precursor3"]
        },
        "characterization_techniques": {
          "reference": ["technique1", "technique2"],
          "test": ["technique1", "technique3"],
          "matches": [
            {
              "reference_item": "technique1",
              "test_item": "technique1",
              "match_value": 1 or 0
            }
          ],
          "total_ground_truth_items": total number of characterization techniques in ground truth data,
          "total_match": total number of matched characterization techniques,
          "missing_items": ["technique2"],
          "extra_items": ["technique3"]
        },
        "steps": {
          "match_value": value between 0 to 1 (float upto two decimal),
          "reference_steps": ["step1", "step2"],
          "test_steps": ["step1", "step3"]
        }
      }
    }

    **Parameters:**
    ground_truth_item
    {ground_truth_item}

    test_item
    {test_item}
  expected_output: >
    A JSON object containing detailed binary evaluation results for synthesis data, with match decisions represented as 1 (match) or 0 (no match).
  agent: synthesis_evaluator_agent
